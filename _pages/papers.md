---
title: "Papers"
permalink: /papers/
author_profile: true
mathjax: true
---

[Preprints](#preprints) · [Publications](#publications) · [Notes](#notes)

## Preprints {#preprints}

- ### <span style="color:#1E90FF; font-weight:bold;">Generalization Below the Edge of Stability: The Role of Data Geometry</span>  
  **Tongtong Liang**, Alexander Cloninger, Rahul Parhi, Yu-Xiang Wang  
  *Manuscript* · 2025  
  <details>
    <summary style="font-weight: bold; color: #0073e6; cursor: pointer;">Abstract</summary>
    <p style="margin-top: 10px; padding-left: 15px;">
      Understanding generalization in overparameterized neural networks hinges on the interplay between data geometry, neural architecture, and training dynamics. This paper gives theoretical results for overparameterized two-layer ReLU networks trained below the edge of stability. For mixtures of low-dimensional balls, we obtain bounds that adapt to intrinsic dimension; for isotropic distributions concentrating toward the sphere, we show rates deteriorate with concentration. The unifying principle is that when data are harder to “shatter” relative to ReLU thresholds, gradient descent learns shared structure and generalizes; near-spherical data encourage memorization.
    </p>
  </details>  
  [<span style="color:#1E90FF;">arXiv</span>](https://arxiv.org/abs/2510.18120)

## Publications {#publications}

- ### <span style="color:#1E90FF; font-weight:bold;">Stable Minima of ReLU Neural Networks Suffer from the Curse of Dimensionality: The Neural Shattering Phenomenon</span>  
  **Tongtong Liang**, Dan Qiao, Yu-Xiang Wang, Rahul Parhi  
  *NeurIPS 2025 (Spotlight)* · 2025  
  <details>
   <summary style="font-weight: bold; color: #0073e6; cursor: pointer;">Abstract</summary>
    <p style="margin-top: 10px; padding-left: 15px;">
      We study the implicit bias of flatness/low curvature and its effects on generalization for two-layer ReLU networks with multivariate inputs. For (1) generalization gap at flat solutions and (2) MSE of stable minima in nonparametric estimation, we prove upper/lower bounds showing rates necessarily deteriorate exponentially with input dimension. A packing construction with boundary-localized ReLU neurons explains how flat solutions can “shatter” with rare activations and large weights, yielding poor high-dimensional performance; simulations corroborate the theory.
    </p>
  </details>  
  [<span style="color:#1E90FF;">arXiv</span>](https://arxiv.org/abs/2506.20779)

## Notes {#notes}

- ### <span style="color:#1E90FF; font-weight:bold;">Power Operations and Formal Group Laws in Complex Cobordism Theory</span>  
  Tongtong Liang · 2022  
  <details>
   <summary style="font-weight: bold; color: #0073e6; cursor: pointer;">Abstract</summary>
    <p style="margin-top: 10px; padding-left: 15px;">
      A survey of Quillen’s approach to complex cobordism, emphasizing how power operations and Landweber–Novikov operations govern the formal group law of \(MU\). It sketches a Riemann–Roch–type formula with fixed-point localization, a homotopical construction via an \(H^\infty\) structure on \(MU\), and a promotion of a lemma of Rudyak from mod-2 to mod-\(p\).
    </p>
  </details>  
  [<span style="color:#1E90FF;">PDF</span>](/files/QuillenSurvey.pdf)

- ### <span style="color:#1E90FF; font-weight:bold;">Obstructions to Realizing Homology Classes by Manifolds</span>  
  Tongtong Liang · 2022  
  <details>
    <summary style="font-weight: bold; color: #0073e6; cursor: pointer;">Abstract</summary>
    <p style="margin-top: 10px; padding-left: 15px;">
      A survey of Thom’s solution to the Steenrod problem: realizing \(H_k(X)\) classes by submanifolds via a lifting problem \(X\to MO(k)\) (or \(MSO(k)\)). It develops \(H^\*(MO(k))\) with Steenrod action, uses Wu’s formula and admissible Sq-bases, applies Whitehead’s theorem to identify the relevant truncations, and derives the dimension bound \(k\le n/2\) and the unoriented case for finite polyhedra.
    </p>
  </details>  
  [<span style="color:#1E90FF;">PDF</span>](/files/ThomSurvey.pdf)
